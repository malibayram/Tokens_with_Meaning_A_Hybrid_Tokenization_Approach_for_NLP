\section{Related Work}
\label{sec:related_work}

Tokenization significantly impacts model performance, especially in morphologically rich languages \cite{toraman_impact_2023}. While subword methods like BPE \cite{sennrich_neural_2016} and WordPiece \cite{schuster_japanese_2012} are standard, they often fail to capture the agglutinative structure of languages like Turkish, Finnish, and Hungarian \cite{baykara_abstractive_2022}.

Early Turkish NLP relied on rule-based morphological analyzers like Zemberek \cite{akin_turk_2007}, which offered precise segmentation but lacked the scalability required for modern LLMs. Recent research has sought to bridge this gap. \citet{toraman_impact_2023} showed that morphological tokenization could recover 97\% of BERT's performance with a fraction of the model size. Similarly, \citet{pan_morphological_2020} and \citet{huck_target_2017} demonstrated that morphology-aware segmentation improves Neural Machine Translation (NMT) by reducing data sparsity. Most recently, \cite{asgari_morphbpe_2025} introduced MorphBPE, a hybrid method that constrains BPE merges to respect morpheme boundaries. Their experiments demonstrated that this linguistic alignment yields tangible computational benefits, resulting in significantly lower training loss and faster convergence rates compared to standard subword models.

Hybrid approaches combining rule-based and statistical methods have shown promise. \citet{kayali_hybrid_2024} used a hybrid tokenizer for Turkish NER and summarization, finding benefits in preserving linguistic structure. \citet{jabbar_morphpiece_2024} introduced MorphPiece for English, achieving superior performance with smaller vocabularies. However, challenges remain in balancing vocabulary size, sequence length, and computational efficiency \cite{henderson_towards_2022, kaya_effect_2024}. Our work extends these efforts by introducing a fully hybrid pipeline that integrates phonological normalization directly into the tokenization process.

Despite the theoretical appeal of morphological segmentation, some studies argue that strict linguistic boundaries may not always be optimal for neural models. \citet{kudo_subword_2018} introduced "subword regularization," suggesting that exposing models to multiple segmentations of the same word (including non-canonical ones) can improve robustness and generalization. Similarly, recent large-scale multilingual models often favor larger, data-driven vocabularies that maximize compression rates over linguistic purity. This "byte-premium" hypothesis suggests that minimizing the number of tokens per word is the primary driver of cross-linguistic performance gaps \cite{martins_eurollm_2024}.

Furthermore, the rise of massive multilingual LLMs presents a dilemma for language-specific tokenization. While a dedicated Turkish tokenizer offers superior alignment for Turkish text, it may not be easily integrated into a model trained on 100+ languages without significantly increasing the combined vocabulary size or complicating the embedding space. Our work acknowledges this tension but argues that for high-stakes or specialized applications in morphologically rich languages, the benefits of semantic coherence and interpretability outweigh the costs of language-specific engineering.


