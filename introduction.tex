\section{Introduction}

Tokenization is a foundational step in Natural Language Processing (NLP), directly impacting vocabulary construction, model efficiency, and downstream task performance \cite{liu_roberta_2019}. While subword-based methods like Byte Pair Encoding (BPE) \citep{sennrich_neural_2016} and WordPiece \citep{schuster_japanese_2012} effectively handle out-of-vocabulary (OOV) words in high-resource languages, they often disregard the linguistic structure of morphologically rich languages. In agglutinative languages like Turkish, Finnish, and Hungarian, words are formed by appending multiple affixes to a root, resulting in complex surface forms. Frequency-based subword models frequently violate morphemic boundaries in these languages, reducing semantic coherence and interpretability \citep{toraman_impact_2023}.

Turkish poses specific challenges due to its agglutinative nature and phonological processes such as vowel harmony and consonant alternation. Words are formed by appending multiple affixes to a root, producing an expansive set of surface forms. For instance, the single word \textit{Avrupalılaştıramadıklarımızdanmışsınızcasına} ("as if you were one of those whom we could not make resemble a European") conveys a meaning that requires an entire sentence in English. Standard tokenizers often treat these variants as distinct or fragment them inconsistently, leading to vocabulary redundancy and poor alignment with linguistic units \cite{bayram_tokenization_2025}. Recent benchmarks, such as TR-MMLU \cite{bayram_setting_2025}, indicate that "token purity"—the alignment of tokens with morphemes—correlates strongly with model performance. Token purity fundamentally determines the clarity of the statistical patterns LLMs learn. Unlike impure subwords that introduce ambiguity, tokens aligned with complete morphemes provide consistent semantic and syntactic signals, facilitating better generalization across complex word forms \citep{hofmann_superbizarre_2021}. Empirical evidence supports this: morphologically informed models have been shown to outperform standard BPE baselines, achieving superior efficiency even with fewer training iterations \cite{jabbar_morphpiece_2024}. This principle mirrors object-centric learning in computer vision, where decomposing inputs into meaningful entities—rather than undifferentiated features—enhances recognition.(capsule networks \cite{sabour_dynamic_2017} and object-centric architectures like Slot Attention \citep{locatello_slot_2020} ) Consequently, token purity is not merely a linguistic preference but a structural necessity for semantic awareness, motivating its use as a primary evaluation metric in this work. 

To address these limitations, we introduce a linguistically informed hybrid tokenization framework. Our approach integrates rule-based morphological segmentation with BPE to ensure both linguistic fidelity and broad coverage. First, phonological processes unify surface variants into shared identifiers. Second, a dedicated <uppercase> token decouples capitalization from lexical identity, preventing vocabulary inflation. Third, explicit formatting tokens are employed to preserve the structural integrity of the input for layout-sensitive tasks. Finally, a hybrid architecture integrates dictionary-based morphological segmentation with BPE, balancing the need for linguistic purity with robust coverage for out-of-vocabulary terms.