\section{Introduction}

Tokenization is a foundational step in Natural Language Processing (NLP), directly impacting vocabulary construction, model efficiency, and downstream task performance \cite{liu_roberta_2019}. While subword-based methods like Byte Pair Encoding (BPE) \citep{sennrich_neural_2016} and WordPiece \citep{schuster_japanese_2012} effectively handle out-of-vocabulary (OOV) words in high-resource languages, they often disregard the linguistic structure of morphologically rich languages. In agglutinative languages like Turkish, Finnish, and Hungarian, words are formed by appending multiple affixes to a root, resulting in complex surface forms. Frequency-based subword models frequently violate morphemic boundaries in these languages, reducing semantic coherence and interpretability \citep{toraman_impact_2023}.

Turkish poses specific challenges due to its agglutinative nature and phonological processes such as vowel harmony and consonant alternation. Words are formed by appending multiple affixes to a root, producing an expansive set of surface forms. For instance, the single word \textit{Avrupalılaştıramadıklarımızdanmışsınızcasına} ("as if you were one of those whom we could not make resemble a European") conveys a meaning that requires an entire sentence in English. Standard tokenizers often treat these variants as distinct or fragment them inconsistently, leading to vocabulary redundancy and poor alignment with linguistic units \cite{bayram_tokenization_2025}. Recent benchmarks, such as TR-MMLU \cite{bayram_setting_2025}, indicate that "token purity"—the alignment of tokens with morphemes—correlates strongly with model performance.

To address these limitations, we introduce a linguistically informed hybrid tokenization framework. Our approach integrates rule-based morphological segmentation with BPE to ensure both linguistic fidelity and broad coverage. Key innovations include: (1) \textbf{Phonological Normalization}, mapping surface variants (e.g., \textit{-dAn}, \textit{-tAn}) to unified token IDs; (2) \textbf{Orthographic Encoding}, using a special \texttt{<uppercase>} token to handle case without vocabulary duplication; and (3) \textbf{Hybrid Fallback}, using BPE only for stems not covered by the morphological dictionary.

We evaluate our tokenizer on the TR-MMLU benchmark, demonstrating significantly higher Turkish Token Percentage (TR~\%) and Pure Token Percentage (Pure~\%) compared to state-of-the-art models like LLaMA, Gemma, and Qwen. These results validate that morphologically aware tokenization yields more semantically meaningful and syntactically coherent representations, offering a pathway to more efficient and equitable multilingual NLP systems.