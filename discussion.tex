\section{Discussion}
\label{sec:discussion}

The current practice in multilingual models (e.g., LLaMA-3, Gemma-2) is to use large, shared vocabularies (128kâ€“256k), which, while efficient for high-resource languages like English, often treats low-resource, agglutinative languages as "second-class citizens" by forcing them into fragmented subword sequences. Our findings propose a viable alternative: a modular tokenization approach or a "mixture-of-tokenizers" architecture, where language-specific morphological adapters are swapped in during training . This method allows models to process each language in its most natural structural form. Crucially, the principles of our hybrid framework are highly portable; they are directly applicable to other agglutinative languages such as Finnish, Hungarian, and Estonian, which share features like rich suffixation and compounding. The underlying "Longest Prefix Match" algorithm with phonological abstraction is language-agnostic, requiring only the replacement of language-specific dictionaries and the definition of phonological rules (like vowel harmony groups). This inherent portability is a significant advantage over purely statistical methods, which demand massive corpus retraining to implicitly discover these linguistic rules.