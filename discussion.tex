\section{Discussion}
\label{sec:discussion}

The results presented in this study highlight a fundamental tension in tokenizer design: the trade-off between vocabulary compactness and morphological fidelity. By prioritizing linguistic structure, our hybrid tokenizer achieves significantly higher alignment with Turkish morphology than standard BPE-based models, but at the cost of increased sequence length.

\subsection{The Efficiency-Expressivity Trade-off}
Our tokenizer generates approximately 63\% more tokens than the Aya-Expanse tokenizer for the same text. In the context of Transformer-based LLMs, where attention complexity scales quadratically with sequence length ($O(N^2)$), this increase implies a higher computational cost during inference. However, this cost must be weighed against the benefits of "expressivity." A model using our tokenizer does not need to learn that \textit{geldim}, \textit{geldin}, and \textit{geldi} are separate entities; it can compositionally derive their meanings from the root \textit{gel-} and the respective suffixes. We hypothesize that this compositional representation could lead to faster convergence during training and better generalization to unseen word forms, effectively shifting the complexity from the vocabulary (memory) to the sequence (compute).

\subsection{Implications for Multilingual Models}
Current multilingual models (e.g., LLaMA-3, Gemma-2) predominantly use large, shared vocabularies (128k-256k) to cover many languages. While efficient for high-resource languages like English, this approach often treats low-resource, agglutinative languages as "second-class citizens," allocating them fewer dedicated tokens and relying on fragmented subword sequences. Our findings suggest that a modular tokenization approach—where language-specific morphological adapters are swapped in during pretraining or fine-tuning—could be a viable alternative. Instead of a single monolithic vocabulary, a "mixture-of-tokenizers" architecture could allow models to process each language in its most natural structural form.

\subsection{Generalizability to Other Languages}
While this study focuses on Turkish, the principles of our hybrid framework are directly applicable to other agglutinative languages such as Finnish, Hungarian, and Estonian. These languages share the core properties of rich suffixation, vowel harmony, and extensive compounding. The "Longest Prefix Match" algorithm with phonological abstraction is language-agnostic; adapting it to Finnish, for example, would primarily require replacing the root and suffix dictionaries and defining the specific phonological rules (e.g., vowel harmony groups) for that language. This portability is a key advantage over purely statistical methods, which require retraining on massive corpora to "discover" these rules implicitly.
