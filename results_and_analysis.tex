\section{Results and Analysis}
\label{sec:results}

We evaluated the proposed tokenizer on the TR-MMLU benchmark \cite{bayram_setting_2025}, comparing it against five state-of-the-art tokenizers: \texttt{google/gemma-2-9b}, \texttt{meta-llama/Llama-3.2-3B}, \texttt{Qwen/Qwen2.5-7B-Instruct}, \texttt{CohereForAI/aya-expanse-8b}, and \texttt{microsoft/phi-4}. Metrics include Vocabulary Size, Total Tokens, Unique Tokens, Turkish Token Percentage (TR~\%), and Pure Token Percentage (Pure~\%).

\begin{table}[h!]
\centering
\caption{Performance comparison on the TR-MMLU dataset.}
\label{tab:turkish_tokenizer_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor[HTML]{DDDDDD} 
\textbf{Tokenizer} & \textbf{Vocab} & \textbf{Tokens} & \textbf{Unique} & \textbf{TR \%} & \textbf{Pure \%} \\ \hline
\texttt{turkish\_tokenizer} (Ours) & 32k & 707k & 11.1k & \textbf{90.29} & \textbf{85.80} \\ \hline
\texttt{google/gemma-2-9b} & 256k & 497k & 6.3k & 40.96 & 28.49 \\ \hline
\texttt{meta-llama/Llama-3.1} & 128k & 488k & 6.8k & 45.77 & 31.45 \\ \hline
\texttt{Qwen/Qwen2.5-7B-Instruct} & 152k & 561k & 5.7k & 40.39 & 30.15 \\ \hline
\texttt{CohereForAI/aya-expanse-8b} & 256k & 434k & 8.5k & 53.48 & 32.96 \\ \hline
\end{tabular}%
}
\end{table}

As shown in Table \ref{tab:turkish_tokenizer_results}, our tokenizer achieves the highest linguistic alignment (90.29\% TR~\%, 85.80\% Pure~\%) while being $8\times$ smaller than Gemma's 256k tokenizer. This size efficiency is crucial, as it significantly reduces the embedding layer parameters, thereby minimizing the model's memory footprint.This linguistic precision leads to an anticipated trade-off: an increased sequence length, with our tokenizer generating 63\% more tokens \texttt{aya-expanse} (707k vs. 434k). While a higher token count means longer sequences and a higher computational cost for attention mechanisms, it provides a crucial benefit for agglutinative languages. It furnishes the model with a more explicit and regular representation of the language, allowing it to learn the compositional rules of the grammar rather than being forced to memorize millions of distinct surface forms (e.g., \textit{geldim, geldin, geldi...}) as individual vocabulary items.

These results confirm that frequency-based tokenizers, even with large vocabularies, struggle with the agglutinative structure of Turkish. Our hybrid approach, by contrast, yields tokens that are linguistically meaningful and consistent, a success that underscores its value for managing this characteristic complexity. By ensuring our tokens are linguistically meaningful and respect morpheme boundaries , we not only achieve higher alignment metrics but also provide the model with a superior, more regular representation of the language. This allows the model to efficiently learn the compositional nature of Turkish grammar, demonstrating that prioritizing morphological integrity can be more effective for complex linguistic structures than simply relying on large, frequency-driven vocabularies.