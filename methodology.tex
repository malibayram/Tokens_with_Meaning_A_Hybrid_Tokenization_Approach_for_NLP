\section{Methodology}
\label{sec:methodology}

Traditional subword tokenization methods like BPE \citep{sennrich_neural_2016} and WordPiece \citep{schuster_japanese_2012} often fail to capture the rich internal structure of agglutinative languages. For example, the Turkish word \textit{anlayabildiklerimizden} is composed of multiple morphemes (\textit{anla-yabil-dik-ler-imiz-den}). Standard tokenizers fragment such words arbitrarily, obscuring grammatical function. To address this, we propose a hybrid tokenization framework that prioritizes morphological segmentation while retaining BPE as a fallback for robustness.

\subsection{Dictionary Construction}
The foundation of our hybrid tokenizer is a curated set of morphological dictionaries derived from open-source linguistic resources, primarily the Zemberek NLP framework \cite{akin_turk_2007} and the Turkish Language Association (TDK) data.
\begin{enumerate}
    \item \textbf{Root Dictionary :} We extracted approximately 22,000 high-frequency Turkish roots (nouns and verbs) from a large-scale corpus of Turkish web text. These roots were filtered to exclude rare or archaic terms that would unnecessarily inflate the vocabulary. Special control tokens (\texttt{<uppercase>}, \texttt{<unknown>}, \texttt{<pad>}, \texttt{<eos>}) were added to support model training.
    \item \textbf{Suffix Dictionary :} This dictionary contains 230 distinct derivational and inflectional suffixes. Crucially, we applied phonological abstraction: suffixes that are phonetically distinct but functionally identical (allomorphs) are mapped to a single canonical ID. For example, the plural suffix forms \textit{-lar} and \textit{-ler} share the same token ID, as do the four variants of the accusative case (\textit{-ı, -i, -u, -ü}). Similarly, the future tense suffixes \textit{-acak} and \textit{-ecek} are unified under a single canonical ID, reflecting their shared grammatical function despite the consonant change.
    \item \textbf{BPE Vocabulary :} To ensure full coverage for foreign words, proper names, and rare scientific terms, we trained a Byte Pair Encoding (BPE) model on the same corpus with a vocabulary limit of 10,000 subwords. This serves as a fallback mechanism for any segment not found in the root or suffix dictionaries.
\end{enumerate}

\subsection{Encoding Process}
Encoding process consists of three main stages when converting text into sequences of IDs. First, the input text is separated into base words by splitting it based on space characters. In this stage, a space prefix is added to the beginning of each word (Example: "elma" → " elma"), due to the fact that all tokens in the tokenizer's roots dictionary include a leading space. Second, these separated words are processed according to their capitalization status. If a word starts with a capital letter, it is represented by a special <uppercase> token followed by the lowercase version of the word (Example: "Ankara" → <uppercase> + " ankara"). If capitalization is not at the beginning (like "iPhone"), the word is fragmented and the capitalized parts are parsed with special tokens. The final stage is converting the pieces into numerical IDs. All obtained pieces (roots, suffixes, and BPE tokens) are searched within the root, suffix, and BPE dictionaries. The search algorithm proceeds by iteratively removing letters backward from the end of the piece (using the longest prefix match logic). For instance, in the word "kitaplar," attempts are made sequentially with "kitaplar," "kitapla," etc.; once the root "kitap" is found in the root list, its ID is added, and then the remaining part, "lar," is searched in the suffix list and its token ID is added. Upon completion of these operations, the final numerical ID list of the text is generated.

\begin{figure}[h]
\centering
\begin{small}
\begin{verbatim}
Algorithm: Hybrid Encoding
Input: text string T
Output: list of token IDs

1. Split T into words by whitespace.
2. For each word W:
    a. Identify uppercase positions.
    b. Split W into segments (handle camelCase).
    c. For each segment S:
        i.   Check ROOTS for longest prefix match.
             If match: add ID, continue.
        ii.  Check SUFFIXES for longest prefix match.
             If match: add ID, continue.
        iii. Check BPE for longest prefix match.
             If match: add ID, continue.
        iv.  Else: add <unknown> ID.
    d. Insert <uppercase> tokens based on (a).
3. Return list of IDs.
\end{verbatim}
\end{small}
\caption{Pseudocode for the hybrid encoding process.}
\label{fig:algo}
\end{figure}

\subsection{Decoding and Phonological Resolution}

The \textbf{Decoding} phase reconstructs text from token IDs using a "Single ID, Multiple Views" principle. Since multiple surface forms (allomorphs) map to a single ID, the decoder must dynamically resolve the correct form based on context. This process is critical for generating natural-sounding Turkish text.

\subsubsection{Root Resolution (Lookahead)}
In this system, a special root selection mechanism is employed in the Decoder stage, following tokenization, to accurately model morphological changes in natural language. Due to the morphological structure of Turkish, surface forms of the same root that have undergone sound events (phonological changes) are represented by the same ID in the system, even though they look different.For instance, kitap (book) and kitab- (the form resulting from consonant softening), ağız (mouth) and ağz- (the form resulting from vowel deletion), and küçük (small) and küçüğ- (the form resulting from consonant deletion/softening) all correspond to the same root ID . As seen in these examples, even though the forms affected by sound events are different on the surface, the model unifies these variants under a single root identity. While the Tokenizer stores these potential variations as a list associated with the ID, the Decoder's main task is to select the correct surface form from this list that is most appropriate for the context. To make this selection, the Decoder calls the specially defined function for every root ID. This function determines the most correct morphological form by looking at the information of the suffix that will follow the root. The main factors the function considers are: whether the next token is a suffix, if it is a suffix, whether it starts with a vowel (like "yor," "acak," or "ı"), whether it is one of certain specific special suffixes, and finally, whether the root's ID falls within a range subject to special rules.

A concrete example of this root selection mechanism is as follows: Let's assume the tokens corresponding to Root ID 100 are → ["sıcak", "sıcağ", "sıca"]. If the ID sequence arrives as [100, 2034] (where ID 2034 represents the suffix "ı"), the Decoder immediately activates the \texttt{\_select\_correct\_root} function. This function detects that ID 2034 is a vowel-initial suffix. Based on this contextual information, the decision is made that consonant softening must occur, and the softened form of the root, "sıcağ," is selected. When the suffix is subsequently added ("sıcağ" + "ı"), the correct morphological form, "sıcağı" (the accusative case of hot/warm), is obtained. This ensures the root is selected and prepared in the correct form based on the type of suffix that will follow it. Additionally, the Decoder manages the Uppercase Marker, which is a special grammatical token. When the token ID is 0, the Decoder understands that this marker must convert the first letter of the immediately following root into a capital letter. 

\subsubsection{Suffix Resolution (Lookbehind)}
In this system, if a token ID is 20,000 or greater, it represents a suffix, and just like roots, any given suffix ID may correspond to multiple surface forms. This is due to the fact that Turkish suffixes follow phonological harmony rules such as major/minor vowel harmony and consonant alternation (e.g., the plural suffix 20000 → [“lar”, “ler”], or the future tense suffix 20030 → [“acak”, “ecek”, “acağ”, “eceğ”, “yacak”, “yecek”, “yacağ”, “yeceğ”]). The primary responsibility of the Decoder is to call a specialized function that selects the correct surface form from these alternative suffix lists based on context. This function evaluates several factors when choosing the appropriate suffix: the last vowel of the preceding word to ensure vowel harmony, whether the final letter of the preceding word is a voiceless consonant for consonant alternation, whether the next token begins with a vowel to determine if a buffer consonant (such as y) is required, the position of the suffix within the word, and whether the suffix belongs to a special category (e.g., “la/le,” “da/de–ta/te,” “cık/cik,” “mak/mek,” “acak/ecek,” etc.). After retrieving the list of surface variants associated with the suffix ID, the function routes the selection process to the appropriate specialized sub-function depending on the suffix category, ensuring that the correct surface form is chosen.

\begin{itemize}
    \item \textbf{Example 1 — Vowel Harmony:} If the preceding word is “ev” (its last vowel “e” is a front vowel), the plural suffix (20000) selects the form “ler” to satisfy front-vowel harmony. Result: [“ev”, “20000”] → “evler”.
    
    \item \textbf{Example 2 — Consonant Assimilation (Fortition):} If the preceding word is “kitap” (ending with the voiceless consonant “p”) and the suffix is -da/-de, the Decoder applies consonant assimilation and hardens the suffix to “ta/te”. Since the last vowel of the word is “a” (a back vowel), the suffix is further matched to the back-vowel form. Result: “kitapta”.
    
    \item \textbf{Example 3 — Future Tense (Vowel Harmony):} If the preceding word is “bak” (its last vowel “a” is a back vowel) and the suffix is -acak/-ecek, the Decoder selects the form “acak” to satisfy back-vowel harmony. Result: “bakacak”.
\end{itemize}