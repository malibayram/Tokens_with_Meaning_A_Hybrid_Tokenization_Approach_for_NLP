\section{Methodology}
\label{sec:methodology}

Traditional subword tokenization methods like BPE \citep{sennrich_neural_2016} and WordPiece \citep{schuster_japanese_2012} often fail to capture the rich internal structure of agglutinative languages. For example, the Turkish word \textit{anlayabildiklerimizden} is composed of multiple morphemes (\textit{anla-yabil-dik-ler-imiz-den}). Standard tokenizers fragment such words arbitrarily, obscuring grammatical function. To address this, we propose a hybrid tokenization framework that prioritizes morphological segmentation while retaining BPE as a fallback for robustness.

\subsection{Dictionary Construction}
The foundation of our hybrid tokenizer is a curated set of morphological dictionaries derived from open-source linguistic resources, primarily the Zemberek NLP framework \cite{akin_turk_2007} and the Turkish Language Association (TDK) data.
\begin{enumerate}
    \item \textbf{Root Dictionary (\texttt{kokler.json}):} We extracted approximately 22,000 high-frequency Turkish roots (nouns and verbs) from a large-scale corpus of Turkish web text. These roots were filtered to exclude rare or archaic terms that would unnecessarily inflate the vocabulary. Special control tokens (\texttt{<uppercase>}, \texttt{<unknown>}, \texttt{<pad>}, \texttt{<eos>}) were added to support model training.
    \item \textbf{Suffix Dictionary (\texttt{ekler.json}):} This dictionary contains 230 distinct derivational and inflectional suffixes. Crucially, we applied phonological abstraction: suffixes that are phonetically distinct but functionally identical (allomorphs) are mapped to a single canonical ID. For example, the plural suffix forms \textit{-lar} and \textit{-ler} share the same token ID, as do the four variants of the accusative case (\textit{-ı, -i, -u, -ü}).
    \item \textbf{BPE Vocabulary (\texttt{bpe\_tokenler.json}):} To ensure full coverage for foreign words, proper names, and rare scientific terms, we trained a Byte Pair Encoding (BPE) model on the same corpus with a vocabulary limit of 10,000 subwords. This serves as a fallback mechanism for any segment not found in the root or suffix dictionaries.
\end{enumerate}

\subsection{Encoding Process}
The encoding phase converts raw text into a sequence of token IDs using a "Longest Prefix Match" algorithm with a strict priority hierarchy: \textbf{Roots $\gg$ Suffixes $\gg$ BPE}. This design ensures that linguistically valid morphemes are always preferred over statistical subwords.

\subsubsection{Handling Special Cases}
\begin{itemize}
    \item \textbf{Case Sensitivity:} Unlike standard lowercasing, we preserve case information using a special \texttt{<uppercase>} token. This token is inserted immediately before any token that was originally capitalized. This approach allows the model to distinguish between proper nouns (e.g., \textit{Ayşe}) and common nouns (e.g., \textit{ayşe}) without duplicating every word in the vocabulary.
    \item \textbf{Acronyms and CamelCase:} Words with mixed casing or all-uppercase acronyms (e.g., \textit{TBMM}, \textit{iPhone}) are first split into segments based on case transitions. Each segment is then tokenized individually. For example, \textit{HTTPServer} is split into \textit{HTTP} and \textit{Server}, which are then processed by the BPE fallback if they are not in the root dictionary.
    \item \textbf{Compound Words:} Lexicalized compounds (e.g., \textit{hanımeli}, \textit{bilgisayar}) are treated as single roots if they appear in the root dictionary. Novel or transparent compounds are naturally segmented into their constituent roots and suffixes by the longest-prefix match algorithm.
\end{itemize}

\begin{figure}[h]
\centering
\begin{small}
\begin{verbatim}
Algorithm: Hybrid Encoding
Input: text string T
Output: list of token IDs

1. Split T into words by whitespace.
2. For each word W:
    a. Identify uppercase positions.
    b. Split W into segments (handle camelCase).
    c. For each segment S:
        i.   Check ROOTS for longest prefix match.
             If match: add ID, continue.
        ii.  Check SUFFIXES for longest prefix match.
             If match: add ID, continue.
        iii. Check BPE for longest prefix match.
             If match: add ID, continue.
        iv.  Else: add <unknown> ID.
    d. Insert <uppercase> tokens based on (a).
3. Return list of IDs.
\end{verbatim}
\end{small}
\caption{Pseudocode for the hybrid encoding process.}
\label{fig:algo}
\end{figure}

\subsection{Decoding and Phonological Resolution}

The \textbf{Decoding} phase reconstructs text from token IDs using a "Single ID, Multiple Views" principle. Since multiple surface forms (allomorphs) map to a single ID, the decoder must dynamically resolve the correct form based on context. This process is critical for generating natural-sounding Turkish text.

\subsubsection{Root Resolution (Lookahead)}
Roots susceptible to alternation are stored with their canonical form but can be modified based on the following suffix.
\begin{itemize}
    \item \textbf{Vowel Softening:} Roots ending in \textit{k, p, ç, t} may soften to \textit{ğ, b, c, d} when followed by a vowel. For example, the root \textit{kitap} (book) is tokenized as a single ID. If the next token is the accusative suffix \textit{-ı}, the decoder outputs \textit{kitabı} instead of \textit{kitapı}.
    \item \textbf{Vowel Dropping:} Some roots lose a vowel when a suffix is added. For instance, \textit{akıl} (mind) + \textit{-ı} becomes \textit{aklı}. The decoder checks the root type and the incoming suffix to apply this transformation.
\end{itemize}

\subsubsection{Suffix Resolution (Lookbehind)}
Suffixes are stored as abstract templates (e.g., \textit{-lAr} for plural) and are instantiated based on the phonological properties of the preceding token.
\begin{itemize}
    \item \textbf{2-Way Vowel Harmony (A-Type):} Suffixes containing \textit{a/e} (e.g., plural \textit{-lar/-ler}) select the vowel based on the last vowel of the previous token.
    \begin{itemize}
        \item Back vowels (\textit{a, ı, o, u}) $\rightarrow$ \textit{-lar} (e.g., \textit{arabalar})
        \item Front vowels (\textit{e, i, ö, ü}) $\rightarrow$ \textit{-ler} (e.g., \textit{evler})
    \end{itemize}
    \item \textbf{4-Way Vowel Harmony (I-Type):} Suffixes containing high vowels (e.g., accusative \textit{-ı/-i/-u/-ü}) select from four variants based on roundness and backness.
    \begin{itemize}
        \item \textit{a, ı} $\rightarrow$ \textit{-ı} (e.g., \textit{kapı-yı})
        \item \textit{e, i} $\rightarrow$ \textit{-i} (e.g., \textit{kedi-yi})
        \item \textit{o, u} $\rightarrow$ \textit{-u} (e.g., \textit{okul-u})
        \item \textit{ö, ü} $\rightarrow$ \textit{-ü} (e.g., \textit{gül-ü})
    \end{itemize}
    \item \textbf{Consonant Hardening:} Suffixes starting with \textit{c, d, g} (e.g., locative \textit{-da}) harden to \textit{ç, t, k} if the previous token ends in a voiceless consonant (F, S, T, K, Ç, Ş, H, P). For example, \textit{sokak} + \textit{-da} $\rightarrow$ \textit{sokakta}.
\end{itemize}

This dynamic resolution allows the vocabulary to remain compact while generating linguistically correct surface forms, effectively decoupling the model's internal representation from the surface complexity of the language.
